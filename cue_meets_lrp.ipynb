{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUE Meets LRP\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vincent-el/cue-meets-lrp/blob/main/cue_meets_lrp.ipynb)\n",
    "\n",
    "I spent an afternoon with [zennit](https://github.com/chr5tphr/zennit) to see how different LRP composite rules explain the same prediction. Three rules, three visual signatures — which is most understandable to a human viewer?\n",
    "\n",
    "To evaluate, I use the [CUE model](https://arxiv.org/abs/2506.14775) (Labarta et al., 2025) as a lens: not just *what* the network saw, but *how legible* each explanation is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: install dependencies, load model and image\n",
    "import subprocess, sys\n",
    "for pkg in [\"zennit\", \"torchvision\"]:\n",
    "    try: __import__(pkg)\n",
    "    except ImportError: subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import urllib.request, io\n",
    "\n",
    "from zennit.composites import EpsilonPlusFlat, EpsilonGammaBox, EpsilonAlpha2Beta1Flat\n",
    "from zennit.attribution import Gradient\n",
    "\n",
    "# Load VGG16\n",
    "model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).eval()\n",
    "labels = VGG16_Weights.IMAGENET1K_V1.meta[\"categories\"]\n",
    "\n",
    "# One image: border collie catching frisbee\n",
    "url = \"https://images.unsplash.com/photo-1503256207526-0d5d80fa2f47?w=640\"\n",
    "req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "img = Image.open(io.BytesIO(urllib.request.urlopen(req).read())).convert(\"RGB\").resize((224, 224))\n",
    "\n",
    "# Preprocess and predict\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "x = preprocess(img).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "    pred_idx = logits.argmax(1).item()\n",
    "    pred_conf = torch.softmax(logits, 1)[0, pred_idx].item()\n",
    "\n",
    "print(f\"Prediction: {labels[pred_idx]} ({pred_conf:.0%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LRP heatmaps with three composite rules\n",
    "composites = {\n",
    "    \"Epsilon+Flat\": EpsilonPlusFlat(),\n",
    "    \"EpsilonGammaBox\": EpsilonGammaBox(low=-3.0, high=3.0),\n",
    "    \"EpsilonAlpha2Beta1\": EpsilonAlpha2Beta1Flat(),\n",
    "}\n",
    "\n",
    "heatmaps = {}\n",
    "x_grad = preprocess(img).unsqueeze(0).requires_grad_(True)\n",
    "target = torch.eye(1000)[[pred_idx]]\n",
    "\n",
    "for name, composite in composites.items():\n",
    "    with Gradient(model=model, composite=composite) as attributor:\n",
    "        _, relevance = attributor(x_grad, target)\n",
    "    heatmaps[name] = relevance.sum(1).squeeze().detach().cpu().numpy()\n",
    "\n",
    "# Visualize: Original + 3 heatmaps in a row\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3.5))\n",
    "\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(f\"{labels[pred_idx]}\\n({pred_conf:.0%})\", fontsize=11)\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "for i, (name, hmap) in enumerate(heatmaps.items()):\n",
    "    vmax = max(abs(hmap.min()), abs(hmap.max()))\n",
    "    axes[i + 1].imshow(hmap, cmap=\"bwr\", vmin=-vmax, vmax=vmax, interpolation=\"bilinear\")\n",
    "    axes[i + 1].set_title(name, fontsize=11)\n",
    "    axes[i + 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Through the CUE Lens\n",
    "\n",
    "The CUE model frames explanation quality as three cognitive stages:\n",
    "\n",
    "- **Legibility**: Can you *see* the signal against the background?\n",
    "- **Readability**: Can you *parse* the structure into coherent regions?\n",
    "- **Interpretability**: Can you *derive meaning* — does the focus make sense?\n",
    "\n",
    "Below I use simple quantitative proxies for each stage. These are rough approximations, not rigorous CUE measurements — but they structure the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_ratio(hmap):\n",
    "    \"\"\"Legibility: How much does the hottest spot stand out?\n",
    "    \n",
    "    Higher = clearer signal against background.\n",
    "    \"\"\"\n",
    "    abs_hmap = np.abs(hmap)\n",
    "    return float(abs_hmap.max() / (np.median(abs_hmap) + 1e-8))\n",
    "\n",
    "def blob_count(hmap, threshold_pct=75):\n",
    "    \"\"\"Readability: How many distinct high-relevance regions?\n",
    "    \n",
    "    Fewer blobs = easier to parse structure.\n",
    "    \"\"\"\n",
    "    abs_hmap = np.abs(hmap)\n",
    "    threshold = np.percentile(abs_hmap, threshold_pct)\n",
    "    binary = abs_hmap >= threshold\n",
    "    labeled, n_blobs = ndimage.label(binary)\n",
    "    return n_blobs\n",
    "\n",
    "def gini(hmap):\n",
    "    \"\"\"Interpretability: How focused is the explanation?\n",
    "    \n",
    "    0 = uniform spread, 1 = all relevance in one pixel.\n",
    "    Higher = more focused, potentially easier to interpret.\n",
    "    \"\"\"\n",
    "    vals = np.abs(hmap).flatten()\n",
    "    vals = np.sort(vals)\n",
    "    n = len(vals)\n",
    "    idx = np.arange(1, n + 1)\n",
    "    return float((2 * np.sum(idx * vals) / (n * np.sum(vals) + 1e-8)) - (n + 1) / n)\n",
    "\n",
    "# Compute scores\n",
    "print(f\"{'Rule':<20} {'Contrast':>10} {'Blobs':>8} {'Gini':>8}\")\n",
    "print(\"-\" * 48)\n",
    "for name, hmap in heatmaps.items():\n",
    "    c = contrast_ratio(hmap)\n",
    "    b = blob_count(hmap)\n",
    "    g = gini(hmap)\n",
    "    print(f\"{name:<20} {c:>10.1f} {b:>8} {g:>8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Vision Model's Perspective\n",
    "\n",
    "Quantitative proxies measure signal properties. As a second lens, I ask a vision-language model which explanation best communicates the prediction — closer to how a human might respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# BLIP-2: stable VLM for image captioning/QA\nfor pkg in [\"transformers\", \"accelerate\"]:\n    try: __import__(pkg)\n    except ImportError: subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndtype = torch.float16 if device == \"cuda\" else torch.float32\n\nprint(f\"Loading BLIP-2 on {device}...\")\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nvlm = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-opt-2.7b\", torch_dtype=dtype\n).to(device).eval()\n\ndef ask_vlm(image, prompt):\n    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, dtype)\n    with torch.no_grad():\n        out = vlm.generate(**inputs, max_new_tokens=50)\n    return processor.decode(out[0], skip_special_tokens=True)\n\n# Convert heatmap to PIL for VLM\ndef heatmap_to_pil(hmap, size=(224, 224)):\n    vmax = max(abs(hmap.min()), abs(hmap.max()))\n    fig, ax = plt.subplots(figsize=(2.24, 2.24), dpi=100)\n    ax.imshow(hmap, cmap=\"bwr\", vmin=-vmax, vmax=vmax, interpolation=\"bilinear\")\n    ax.axis(\"off\")\n    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n    buf = io.BytesIO()\n    fig.savefig(buf, format=\"PNG\", bbox_inches=\"tight\", pad_inches=0)\n    plt.close(fig)\n    buf.seek(0)\n    return Image.open(buf).convert(\"RGB\").resize(size)\n\n# Ask about each heatmap\nprint(f\"\\nAsking BLIP-2 about each explanation...\\n\")\nfor name, hmap in heatmaps.items():\n    hmap_img = heatmap_to_pil(hmap)\n    prompt = f\"Question: This heatmap shows which image regions led to classifying something as '{labels[pred_idx]}'. Red supports the prediction, blue contradicts. Is the focus clear and concentrated, or scattered? Answer:\"\n    response = ask_vlm(hmap_img, prompt)\n    print(f\"{name}:\\n  {response.strip()}\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "The quantitative proxies and VLM give different perspectives on the same heatmaps. Neither is a substitute for asking real humans — which is what the CUE framework ultimately calls for.\n",
    "\n",
    "This is the question I want to learn to answer properly: *what makes an explanation actually land?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I'd Explore Next\n",
    "\n",
    "This demo evaluates *static* explanations — one image, one prediction, one heatmap.\n",
    "\n",
    "But many AI systems make *sequential* decisions: a robot navigating, an agent learning a policy, a trajectory unfolding over time. Explaining these requires not \"which pixels?\" but \"which past observations shaped this action?\" — temporal relevance, possibly counterfactual reasoning.\n",
    "\n",
    "The CUE lens still applies: can you *trace* the sequence (legibility), *parse* the causal structure (readability), *understand* the strategy (interpretability)? But the *form* of explanation changes. That's the frontier I'm most curious about.\n",
    "\n",
    "---\n",
    "\n",
    "*Built with [zennit](https://github.com/chr5tphr/zennit) (Fraunhofer HHI) · Vincent Lange · vincentelange@gmail.com*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}